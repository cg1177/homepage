<!doctype html>
<html lang="en-US">
<link rel="stylesheet" href="resources/styles.css">
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172325941-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-172325941-2');
  </script>

  <style>
        #sortableTable {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: #fff;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            font-size: 14px;
        }
        #sortableTable th, #sortableTable td {
            padding: 8px;
            border-bottom: 1px solid #eee;
            transition: all 0.2s ease;
        }
        #sortableTable th {
            cursor: pointer;
            background-color: #4a5568;
            color: white;
            position: relative;
            transition: all 0.3s ease;
        }
        #sortableTable th:hover {
            background-color: #718096;
            transform: translateY(-2px);
        }
        #sortableTable th.sorted-asc::after {
            content: " ▲";
            position: absolute;
            right: 0px;
            animation: fadeIn 0.5s ease;
        }
        #sortableTable th.sorted-desc::after {
            content: " ▼";
            position: absolute;
            right: 0px;
            animation: fadeIn 0.5s ease;
        }
        #sortableTable tr:nth-child(even) {
            background-color: #f7f9fc;
        }
        #sortableTable tr:hover {
            background-color: #e8f4f8;
            transform: scale(1.02);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        /* 为不同区域设置不同的背景色 */
        /* Rank */
        #sortableTable td:nth-child(1) {
            background-color: #f8fafc;  /* 浅灰色 */
        }
        
        /* Models */
        #sortableTable td:nth-child(2) {
            background-color: #e8eef9;  /* 浅蓝灰色 */
        }
        
        /* LLM & #F (columns 3-5) */
        #sortableTable td:nth-child(3),
        #sortableTable td:nth-child(4),
        #sortableTable td:nth-child(5) {
            background-color: #edf2f7;  /* 浅灰色 */
        }
        
        /* MCQ (columns 6-7) */
        #sortableTable td:nth-child(6),
        #sortableTable td:nth-child(7) {
            background-color: #e6ffed;  /* 非常浅的绿色 */
        }
        
        /* Cred. Eval. (columns 8-11) */
        #sortableTable td:nth-child(8),
        #sortableTable td:nth-child(9),
        #sortableTable td:nth-child(10),
        #sortableTable td:nth-child(11) {
            background-color: #fff1f0;  /* 非常浅的红色 */
        }
        
        /* Open-Ended (column 12) */
        #sortableTable td:nth-child(12) {
            background-color: #fff7e6;  /* 非常浅的橙色 */
        }

        /* 奇数行稍微深一点的背景色 */
        #sortableTable tr:nth-child(odd) td {
            filter: brightness(0.98);
        }

        /* Hover效果 */
        #sortableTable tr:hover td {
            filter: brightness(0.95);
            transition: all 0.3s ease;
        }

        /* 表头基础样式 */
        #sortableTable thead th {
            padding: 12px;
            font-weight: 500;
            text-align: center;
            border: none;
            position: sticky;
            top: 0;
            z-index: 1;
            color: white;
        }

        /* 不同区域的表头样式 */
        /* Rank & Models */
        #sortableTable thead tr:first-child th:nth-child(1),
        #sortableTable thead tr:first-child th:nth-child(2) { 
            background-color: #1a202c;  /* 深蓝黑色 */
        }

        /* LLM & #F 区域 */
        #sortableTable thead tr:first-child th[colspan="1"]:nth-of-type(3),
        #sortableTable thead tr:first-child th[colspan="2"]:nth-of-type(4),
        #sortableTable thead tr:nth-child(2) th:nth-child(3) {
            background-color: #4a5568;  /* 深灰色 */
        }

        /* MCQ 区域 */
        #sortableTable thead tr:first-child th[colspan="2"]:nth-of-type(5),
        #sortableTable thead tr:nth-child(2) th:nth-child(4),
        #sortableTable thead tr:nth-child(2) th:nth-child(5) {
            background-color: #3c8a5f;  /* 深绿色 */
        }

        /* Cred. Eval. 区域 */
        #sortableTable thead tr:first-child th[colspan="4"],
        #sortableTable thead tr:nth-child(2) th:nth-child(6),
        #sortableTable thead tr:nth-child(2) th:nth-child(7),
        #sortableTable thead tr:nth-child(2) th:nth-child(8),
        #sortableTable thead tr:nth-child(2) th:nth-child(9) {
            background-color: #9b2c2c;  /* 深红色 */
        }

        /* Open-Ended 区域 */
        #sortableTable thead tr:first-child th:last-child,
        #sortableTable thead tr:nth-child(2) th:last-child {
            background-color: #9c4221;  /* 深棕色 */
        }

        /* 表头悬停效果 */
        #sortableTable thead th:hover {
            filter: brightness(1.1);
            transition: all 0.3s ease;
        }

        /* 排序指示器样式 */
        #sortableTable th.sorted-asc::after,
        #sortableTable th.sorted-desc::after {
            margin-left: 8px;
            font-size: 0.8em;
        }

        /* 确保表格本身的样式配合表头 */
        #sortableTable {
            border-collapse: collapse;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        /* 移除hover效果 */
        #sortableTable th:hover {
            transform: none;
            background-color: inherit;
        }

        /* 添加行动画相关样式 */
        #sortableTable tbody tr {
            transition: all 0.3s ease;
        }

        /* 添加行交换动画的类 */
        .row-transition {
            animation: rowMove 0.5s ease;
        }

        @keyframes rowMove {
            0% { opacity: 0.5; transform: translateY(-10px); }
            100% { opacity: 1; transform: translateY(0); }
        }

        /* 为所有图片添加过渡效果 */
        img {
            transition: transform 0.3s ease;
            display: block;  /* 确保margin: auto生效 */
            max-width: 100%;  /* 保持响应式 */
        }

        /* 鼠标悬停时的放大效果 */
        img:hover {
            transform: scale(1.3);  /* 放大到原始大小的1.3倍 */
            transition-delay: 0.6s;  /* 添加1秒延迟 */
        }

        /* 可选：添加平滑阴影效果增强视觉体验 */
        img {
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        img:hover {
            transform: scale(1.3);
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            transition-delay: 0.6s;  /* 阴影效果也延迟1秒 */
        }

        /* 为前三设置不同的高度、字体大小和粗细的过渡效果 */
        #sortableTable tbody tr:nth-child(1) {
            height: 80px;  /* 第一行最高 */
            background: linear-gradient(to bottom, rgba(255,255,255,0.1) 0%, rgba(255,255,255,0) 100%);
            transition: all 0.3s ease;
        }
        
        #sortableTable tbody tr:nth-child(1) td {
            font-size: 18px;  /* 第一行字体最大 */
            font-weight: 700;  /* 最粗 */
            color: #000;  /* 更深的颜色 */
        }
        
        #sortableTable tbody tr:nth-child(2) {
            height: 65px;  /* 第二行次高 */
            background: linear-gradient(to bottom, rgba(255,255,255,0.05) 0%, rgba(255,255,255,0) 100%);
            transition: all 0.3s ease;
        }
        
        #sortableTable tbody tr:nth-child(2) td {
            font-size: 16px;  /* 第二行字体稍小 */
            font-weight: 600;  /* 次粗 */
            color: #1a1a1a;
        }
        
        #sortableTable tbody tr:nth-child(3) {
            height: 50px;  /* 第三行最接近普通行高 */
            background: linear-gradient(to bottom, rgba(255,255,255,0.02) 0%, rgba(255,255,255,0) 100%);
            transition: all 0.3s ease;
        }
        
        #sortableTable tbody tr:nth-child(3) td {
            font-size: 15px;  /* 第三行字体更接近普通大小 */
            font-weight: 500;  /* 中等粗细 */
            color: #333;
        }

        /* 确保其他行保持正常样式 */
        #sortableTable tbody tr {
            height: 40px;  /* 普通行的高度 */
            transition: all 0.3s ease;
        }
        
        #sortableTable tbody tr td {
            font-size: 14px;  /* 普通行的字体大小 */
            font-weight: 400;  /* 普通粗细 */
            color: #444;
            vertical-align: middle;
            transition: all 0.3s ease;
        }

        /* 保持单元格内容垂直居中 */
        #sortableTable tbody td {
            vertical-align: middle;
        }

        .name-link {
            font-size: 18px;
            font-weight: bold;
            color: #2d3748;
            text-decoration: none;
            transition: color 0.3s ease, transform 0.3s ease;
            display: inline-block;
            position: relative;
        }

        .name-link::after {
            content: '';
            position: absolute;
            width: 100%;
            height: 2px;
            background-color: #3182ce;
            left: 0;
            bottom: -5px;
            transform: scaleX(0);
            transition: transform 0.3s ease;
        }

        .name-link:hover {
            color: #3182ce;
            transform: translateY(-3px);
        }

        .name-link:hover::after {
            transform: scaleX(1);
        }

        .button {
            font-size: 16px;
            font-weight: bold;
            color: white;
            background-color: #3182ce;
            border: none;
            border-radius: 5px;
            padding: 10px 20px;
            text-decoration: none;
            transition: background-color 0.3s ease, transform 0.3s ease;
            display: inline-block;
            margin: 5px;
        }

        .button:hover {
            background-color: #2b6cb0;
            transform: translateY(-3px);
        }

        .styled-button {
            font-size: 20px;
            font-weight: bold;
            color: #000000; /* 黑色文字 */
            background-color: #ffffff; /* 白色背景 */
            border: 2px solid #3182ce; /* 蓝色边框 */
            border-radius: 5px;
            padding: 10px 20px;
            text-decoration: none;
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.3s ease;
            display: inline-block;
            margin: 5px;
        }

        .styled-button:hover {
            background-color: #2b6cb0; /* 深蓝色背景 */
            color: #ffffff; /* 白色文字 */
            transform: translateY(-3px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }

        .fade-in {
            opacity: 0;
            transform: translateY(20px);
            transition: all 0.6s ease-out;
        }

        .fade-in.visible {
            opacity: 1;
            transform: translateY(0);
        }

        a {
            transition: all 0.3s ease;
            position: relative;
        }

        a:hover {
            color: #3182ce;
        }

        a::after {
            content: '';
            position: absolute;
            width: 100%;
            height: 2px;
            bottom: -2px;
            left: 0;
            background-color: #3182ce;
            transform: scaleX(0);
            transition: transform 0.3s ease;
        }

        a:hover::after {
            transform: scaleX(1);
        }
  </style>

<div max-width=100%>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <title>CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://chenguo.netlify.app/projects/cg_bench" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="CG-Bench" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta property="og:description" content="Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang. CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta property="og:url" content="https://chenguo.netlify.app/projects/cg_bench" />

  <meta property="article:publisher" content="https://github.com/cg1177" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta name="twitter:description" content="Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang. CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta name="twitter:url" content="https://chenguo.netlify.app/projects/cg_bench" />
  <meta name="twitter:site" content="@ajayj_" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />
</head>

<body>
      <script>
        window.onload = function() {
            sortTable(6);
        }
      </script>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">CG-Bench: Clue-grounded Question Answering Benchmark <br>for Long Video Understanding</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:1000px;margin:auto;">
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=lRj3moAAAAAJ" target="_blank">Guo Chen*</a></span></center></div>
          <div><center><span><a class="name-link" href="/" target="_blank">Yicheng Liu</a></span></center></div>
          <div><center><span><a class="name-link" href="https://hyf015.github.io/" target="_blank">Yifei Huang</a></span></center></div>
          <div><center><span><a class="name-link" href="" target="_blank">Yuping He</a></span></center></div>
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=sTCkd54AAAAJ" target="_blank">Baoqi Pei</a></span></center></div>
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=mf2U64IAAAAJ" target="_blank">Jilan Xu</a></span></center></div>
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=hD948dkAAAAJ" target="_blank">Yali Wang</a></span></center></div>
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=HEuN8PcAAAAJ" target="_blank">Limin Wang</a></span></center></div>
          <div><center><span><a class="name-link" href="https://scholar.google.com/citations?user=mgqhQGkAAAAJ" target="_blank">Tong Lu</a></span></center></div>
    
      </div>

      <center><span style="font-size:24px;">Nanjing University</span></center>

      <!-- <center><span style="font-size:20px;"><a href='https://iccv2023.thecvf.com/'>International Conference
        on Computer Vision (ICCV), 2023</a></span></center> -->

      <div class="table-like" style="justify-content:space-evenly;max-width:800px;margin:auto;padding:5px">
        <div><center><a class="styled-button" href='https://arxiv.org/abs/2308.07893'><i class="fas fa-file-alt"></i> [Paper]</a></center></div>
        <div><center><a class="styled-button" href='https://huggingface.co/datasets/CG-Bench/CG-Bench'><i class="fas fa-database"></i> [Dataset]</a></center></div>
        <div><center><a class="styled-button" href='https://github.com/cg1177/CG-Bench'><i class="fab fa-github"></i> [GitHub]</a></center></div>
        <div><center><a class="styled-button" href='#leaderboard'><i class="fas fa-trophy"></i> [Leaderboard]</a></center></div>
      </div>

      <div style="width:1000px; margin:0 auto; text-align=right;">
      <p>
      <b>Summary:</b> We introduce CG-Bench, a groundbreaking benchmark for clue-grounded question answering in long videos, addressing the limitations of existing benchmarks that focus primarily on short videos and rely on multiple-choice questions (MCQs). These limitations allow models to answer by elimination rather than genuine understanding. CG-Bench enhances evaluation credibility by requiring models to retrieve relevant clues for questions. It includes 1,219 manually curated videos across 14 primary, 171 secondary, and 638 tertiary categories, making it the largest benchmark for long video analysis. With 12,129 QA pairs in perception, reasoning, and hallucination question types, CG-Bench introduces innovative clue-based evaluation methods: clue-grounded white box and black box evaluations, ensuring answers are based on correct video understanding. Evaluations of various MLLMs reveal significant performance gaps in long video comprehension, especially between open-source and commercial models. We aim for CG-Bench to drive the development of more reliable and capable MLLMs for long video understanding. All annotations and video data will be publicly released.
    </p>
      <a href="resources/teaser.png"><img width="1000" max-width="100%" src="resources/teaser.png" /></a>
      <!-- <p>
        <b>Abstract:</b> Most existing video understanding benchmarks for multimodal large language models (MLLMs) focus only on short videos. The limited number of benchmarks for long video understanding often rely solely on multiple-choice questions (MCQs). However, because of the inherent limitation of MCQ-based evaluation and the increasing reasoning ability of MLLMs, models can give the current answer purely by combining short video understanding with elimination, without genuinely understanding the video content.
           To address this gap, we introduce CG-Bench, a novel benchmark designed for clue-grounded question answering in long videos. CG-Bench emphasizes the model's ability to retrieve relevant clues for questions, enhancing evaluation credibility. It features 1,219 manually curated videos categorized by a granular system with 14 primary categories, 171 secondary categories, and 638 tertiary categories, making it the largest benchmark for long video analysis. The benchmark includes 12,129 QA pairs in three major question types: perception, reasoning, and hallucination.
           Compensating the drawbacks of pure MCQ-based evaluation, we design two novel clue-based evaluation methods: clue-grounded white box and black box evaluations, to assess whether the model generates answers based on the correct understanding of the video.
           We evaluate multiple closed-source and open-source MLLMs on CG-Bench. Results indicate that current models significantly underperform in understanding long videos compared to short ones, and a significant gap exists between open-source and commercial models. We hope CG-Bench can advance the development of more trustworthy and capable MLLMs for long video understanding. All annotations and video data will be publicly released.
      </p> -->
      </div>
      <center>
      <br><hr>
      
      <div class="leaderboard-header" id="leaderboard">
        <h1>Leaderboard</h1>
      </div>

      <div class="font-size-controls">
        <button onclick="adjustFontSize('increase')" class="font-button">
          <span>A+</span>
        </button>
        <button onclick="adjustFontSize('decrease')" class="font-button">
          <span>A-</span>
        </button>
      </div>

      <style>
        .leaderboard-header {
          max-width: 1200px;
          margin: 0 auto;
          padding: 20px;
          text-align: center;
        }

        .leaderboard-header h1 {
          font-size: 48px;
          color: #2d3748;
          margin-bottom: 30px;
          font-weight: 700;
          text-transform: uppercase;
          letter-spacing: 2px;
        }

        .leaderboard-description {
          max-width: 1200px;
          margin: 0 auto;
          color: #4a5568;
          line-height: 1.6;
          font-size: 17px;
          text-align: left;
          background: #f7fafc;
          padding: 20px 30px;
          border-radius: 8px;
          box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .leaderboard-description ul {
          margin: 15px 0;
          padding-left: 20px;
        }

        .leaderboard-description li {
          margin: 8px 0;
        }

        .leaderboard-description strong {
          color: #2d3748;
        }

        .font-size-controls {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            display: flex;
            flex-direction: column;
            gap: 10px;
            z-index: 1000;
        }

        .font-button {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            border: none;
            background-color: #4a5568;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
            transition: all 0.3s ease;
        }

        .font-button:hover {
            background-color: #2d3748;
            transform: scale(1.1);
        }

        .font-button span {
            font-size: 16px;
        }
      </style>

      <div style="width:1400px; margin:0 auto; text-align: center">
        <table id="sortableTable">
          <thead>
              <tr>
                  <th rowspan="2">Rank</th>
                  <th rowspan="2">Models</th>
                  <th colspan="1">LLM</th>
                  <th colspan="2">#F</th>
                  <th colspan="2">MCQ</th>
                  <th colspan="4">Cred. Eval.</th>
                  <th rowspan="1">Open-Ended</th>
              </tr>
              <tr>
                <!-- <th onclick="sortTable(0)">Models</th> -->
                  <th>#param</th>
                  <th>clue</th>
                  <th>long</th>
                  <th onclick="sortTable(5)">clue-acc.</th>
                  <th onclick="sortTable(6)">long-acc.</th>
                  <th onclick="sortTable(7)">mIoU</th>
                  <th onclick="sortTable(8)">rec.@IoU</th>
                  <th onclick="sortTable(9)">acc.@IoU</th>
                  <th onclick="sortTable(10)">CRR</th>
                  <th onclick="sortTable(11)">acc.</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>1</td>
                  <td>Videochat2 (Li et al., 2023b)</td>
                  <td>7B</td>
                  <td>16</td>
                  <td>16</td>
                  <td>48.5</td>
                  <td>30.1</td>
                  <td>3.03</td>
                  <td>2.4</td>
                  <td>8.31</td>
                  <td>62.1</td>
                  <td>8.1</td>
              </tr>
              <tr>
                  <td>2</td>
                  <td>VideoLLAMA (Zhang et al., 2023)</td>
                  <td>7B</td>
                  <td>32</td>
                  <td>32</td>
                  <td>49.2</td>
                  <td>31.2</td>
                  <td>3.67</td>
                  <td>3.2</td>
                  <td>9.1</td>
                  <td>63.4</td>
                  <td>9.1</td>
              </tr>
              <tr>
                  <td>3</td>
                  <td>Video-LLAVA (Lin et al., 2023)</td>
                  <td>7B</td>
                  <td>16</td>
                  <td>16</td>
                  <td>49.7</td>
                  <td>30.6</td>
                  <td>3.41</td>
                  <td>3.8</td>
                  <td>9.9</td>
                  <td>61.6</td>
                  <td>10.1</td>
              </tr>
              <tr>
                  <td>4</td>
                  <td>ST-LLM (Liu et al., 2024c)</td>
                  <td>7B</td>
                  <td>32</td>
                  <td>32</td>
                  <td>50.2</td>
                  <td>31.3</td>
                  <td>3.19</td>
                  <td>3.0</td>
                  <td>10.3</td>
                  <td>62.4</td>
                  <td>11.4</td>
              </tr>
              <tr>
                  <td>5</td>
                  <td>Chat-UniVi-v1.5 (Jin et al., 2024)</td>
                  <td>13B</td>
                  <td>32</td>
                  <td>64</td>
                  <td>50.6</td>
                  <td>33.1</td>
                  <td>4.45</td>
                  <td>5.1</td>
                  <td>11.4</td>
                  <td>65.4</td>
                  <td>12.9</td>
              </tr>
              <tr>
                  <td>6</td>
                  <td>ShareGPT4Video (Chen et al., 2024a)</td>
                  <td>16B</td>
                  <td>16</td>
                  <td>16</td>
                  <td>51.3</td>
                  <td>33.6</td>
                  <td>4.59</td>
                  <td>4.8</td>
                  <td>10.8</td>
                  <td>67.5</td>
                  <td>13.6</td>
              </tr>
              <tr>
                  <td>7</td>
                  <td>Qwen-VL-Chat (Bai et al., 2023)</td>
                  <td>7B</td>
                  <td>16</td>
                  <td>16</td>
                  <td>51.3</td>
                  <td>34.1</td>
                  <td>4.30</td>
                  <td>5.1</td>
                  <td>11.7</td>
                  <td>66.5</td>
                  <td>10.8</td>
              </tr>
              <tr>
                  <td>8</td>
                  <td>ViLA (Lin et al., 2024)</td>
                  <td>8B</td>
                  <td>14</td>
                  <td>14</td>
                  <td>53.2</td>
                  <td>34.6</td>
                  <td>4.82</td>
                  <td>4.6</td>
                  <td>11.5</td>
                  <td>65.0</td>
                  <td>9.4</td>
              </tr>
              <tr>
                  <td>9</td>
                  <td>GroundVQA (Liu et al., 2024d)</td>
                  <td>0.25B</td>
                  <td>-</td>
                  <td>1200</td>
                  <td>27.3</td>
                  <td>27.3</td>
                  <td>3.32</td>
                  <td>3.5</td>
                  <td>10.3</td>
                  <td>59.2</td>
                  <td>7.2</td>
              </tr>
              <tr>
                  <td>10</td>
                  <td>GeLM (Chen et al., 2024c)</td>
                  <td>7B</td>
                  <td>-</td>
                  <td>100</td>
                  <td>-</td>
                  <td>-</td>
                  <td>5.15</td>
                  <td>4.2</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
              </tr>
              <tr>
                  <td>11</td>
                  <td>ET-Chat (Liu et al., 2024)</td>
                  <td>4B</td>
                  <td>-</td>
                  <td>1fps</td>
                  <td>-</td>
                  <td>-</td>
                  <td>2.33</td>
                  <td>2.3</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
              </tr>
              <tr>
                  <td>12</td>
                  <td>InternVL-Chat-v1.5 (Chen et al., 2023d)</td>
                  <td>20B</td>
                  <td>10</td>
                  <td>10</td>
                  <td>54.6</td>
                  <td>36.3</td>
                  <td>4.24</td>
                  <td>4.6</td>
                  <td>14.2</td>
                  <td>66.5</td>
                  <td>9.2</td>
              </tr>
              <tr>
                  <td>13</td>
                  <td>MiniCPM-v2.6 (Yao et al., 2024)</td>
                  <td>8B</td>
                  <td>32</td>
                  <td>64</td>
                  <td>53.4</td>
                  <td>35.4</td>
                  <td>5.03</td>
                  <td>4.4</td>
                  <td>12.2</td>
                  <td>66.3</td>
                  <td>13.6</td>
              </tr>
              <tr>
                  <td>14</td>
                  <td>InternVL2 (Chen et al., 2024e)</td>
                  <td>34B</td>
                  <td>16</td>
                  <td>32</td>
                  <td>57.3</td>
                  <td>38.9</td>
                  <td>4.53</td>
                  <td>4.3</td>
                  <td>15.1</td>
                  <td>67.9</td>
                  <td>14.4</td>
              </tr>
              <tr>
                  <td>15</td>
                  <td>Kangaroo (Liu et al., 2024b)</td>
                  <td>8B</td>
                  <td>32</td>
                  <td>64</td>
                  <td>54.2</td>
                  <td>36.8</td>
                  <td>5.12</td>
                  <td>5.8</td>
                  <td>12.8</td>
                  <td>67.9</td>
                  <td>14.4</td>
              </tr>
              <tr>
                  <td>16</td>
                  <td>LLaVA-One Vision (Li et al., 2024)</td>
                  <td>72B</td>
                  <td>32</td>
                  <td>32</td>
                  <td>60.2</td>
                  <td>40.7</td>
                  <td>6.14</td>
                  <td>5.3</td>
                  <td>15.3</td>
                  <td>67.7</td>
                  <td>17.6</td>
              </tr>
              <tr>
                  <td>17</td>
                  <td>Video-CCAM (Fei et al., 2024)</td>
                  <td>14B</td>
                  <td>32</td>
                  <td>32</td>
                  <td>57.2</td>
                  <td>35.7</td>
                  <td>5.29</td>
                  <td>5.1</td>
                  <td>13.6</td>
                  <td>67.1</td>
                  <td>14.9</td>
              </tr>
              <tr>
                  <td>18</td>
                  <td>LongVA (Zhang et al., 2024a)</td>
                  <td>7B</td>
                  <td>32</td>
                  <td>64</td>
                  <td>53.6</td>
                  <td>36.9</td>
                  <td>5.21</td>
                  <td>6.0</td>
                  <td>12.7</td>
                  <td>68.8</td>
                  <td>14.4</td>
              </tr>
              <tr>
                  <td>19</td>
                  <td>VITA (Fu et al., 2024b)</td>
                  <td>8x7B</td>
                  <td>32</td>
                  <td>32</td>
                  <td>59.1</td>
                  <td>40.3</td>
                  <td>5.85</td>
                  <td>6.4</td>
                  <td>15.8</td>
                  <td>68.8</td>
                  <td>16.9</td>
              </tr>
              <tr>
                  <td>20</td>
                  <td>Qwen2-VL (Wang et al., 2024b)</td>
                  <td>72B</td>
                  <td>32</td>
                  <td>128</td>
                  <td>65.4</td>
                  <td>51.4</td>
                  <td>7.11</td>
                  <td>6.5</td>
                  <td>17.1</td>
                  <td>78.6</td>
                  <td>25.2</td>
              </tr>
              <tr>
                  <td>21</td>
                  <td>GPT-4o-08-06 (OpenAI, 2024)</td>
                  <td>-</td>
                  <td>32</td>
                  <td>128</td>
                  <td>66.5</td>
                  <td>53.9</td>
                  <td>8.33</td>
                  <td>12.3</td>
                  <td>21.7</td>
                  <td>81.1</td>
                  <td>37.2</td>
              </tr>
              <tr>
                  <td>22</td>
                  <td>GPT-4o-mini-07-25 (OpenAI, 2024)</td>
                  <td>-</td>
                  <td>32</td>
                  <td>128</td>
                  <td>56.0</td>
                  <td>41.9</td>
                  <td>5.56</td>
                  <td>6.5</td>
                  <td>15.2</td>
                  <td>74.8</td>
                  <td>23.8</td>
              </tr>
              <tr>
                  <td>23</td>
                  <td>Gemini-1.5-Pro (Anil et al., 2023)</td>
                  <td>-</td>
                  <td>32</td>
                  <td>128</td>
                  <td>61.6</td>
                  <td>43.4</td>
                  <td>7.56</td>
                  <td>11.4</td>
                  <td>18.6</td>
                  <td>75.6</td>
                  <td>26.9</td>
              </tr>
              <tr>
                  <td>24</td>
                  <td>Gemini-1.5-Flash (Anil et al., 2023)</td>
                  <td>-</td>
                  <td>32</td>
                  <td>128</td>
                  <td>60.0</td>
                  <td>42.0</td>
                  <td>5.83</td>
                  <td>7.0</td>
                  <td>14.9</td>
                  <td>70.0</td>
                  <td>16.5</td>
              </tr>
              <tr>
                  <td>25</td>
                  <td>Claude-3.5-Sonnet</td>
                  <td>-</td>
                  <td>20</td>
                  <td>20</td>
                  <td>61.7</td>
                  <td>39.8</td>
                  <td>5.09</td>
                  <td>5.5</td>
                  <td>12.3</td>
                  <td>64.5</td>
                  <td>10.3</td>
              </tr>
          </tbody>
      </table>
      </div>
      <div class="leaderboard-header">
        <div class="leaderboard-description">
          <p>This leaderboard tracks the performance of different models on the <b>mini-set</b> of CG-Bench dataset. It contains 1118 videos and 3000 questions for fast evaluation. The evaluation metrics include:</p>
          <ul>
            <li><strong>MCQ:</strong> Multiple Choice Questions accuracy on clue-based and long-video settings:
              <ul>
                <li><strong>clue-acc:</strong> Accuracy on multiple choice questions when only given the relevant video clue segments</li>
                <li><strong>long-acc:</strong> Accuracy on multiple choice questions when processing the entire long video</li>
              </ul>
            </li>
            
            <li><strong>Cred. Eval:</strong> Credibility evaluation metrics including:
              <ul>
                <li><strong>mIoU:</strong> Mean Intersection over Union - measures the overlap between predicted and ground truth clue intervals</li>
                <li><strong>recall@IoU:</strong> Percentage of predictions with IoU above threshold - evaluates model's ability to locate relevant clues</li>
                <li><strong>accuracy@IoU:</strong> Answer accuracy when IoU exceeds threshold - assesses answer quality with correct clue identification</li>
                <li><strong>CRR:</strong> Clue Retrieval Rate - measures model's overall effectiveness in finding relevant video segments</li>
              </ul>
            </li>
            <li><strong>Open-Ended:</strong> Performance on open-ended question answering
              <ul>
                <li><strong>acc:</strong> Accuracy on open-ended questions</li>
              </ul>
            </li>
          </ul>
          <p>Click on column headers to sort by different metrics.</p>
          <p>If you want to submit your model, please fill in this <strong><a href="https://docs.google.com/forms/d/1HVoj9TdneApbJgXZONxWfsEeSTBHHm0TVcxtZjyzl9U/edit">sheet</a></strong>.</p>
        </div>
      </div>


      <!-- <br />
      <hr> -->

      <div style="width:1200px; margin:0 auto; text-align=center">
        <center><h1>Benchmark Statistics</h1></center>
      </div>
      <div style="width:1200px; margin:0 auto; text-align: left">
        <b>Video Meta:</b> Our dataset comprises a total of 1219 videos with multiple multimodal information, including vision, audio, and subtitles. The duration of the videos varies between 10 and 80 minutes. Notably, videos that last between 20 and 30 minutes are the most prevalent. This selection process is manual, based on content relevance, which mirrors real-world duration distributions and highlights a long-tail effect for longer videos.
        As illustrated in Figure 2, each video is classified using a three-tiered tagging system that succinctly encapsulates its content and assigns it to fundamental categories. The primary classification is augmented by a secondary layer of 171 tags and a tertiary layer consisting of 638 tags. This multi-level tagging mechanism guarantees a broad diversity of data content. For a more detailed classification of tags, please consult the supplementary materials.
        <br>
        <b>Question Meta:</b> We annotate it with high-quality question-answer-clue (QAC) triplets. To ensure question diversity, we first establish a taxonomy with three main types: Perception, Reasoning, and Hallucination. As shown in Figure 3, Perception and Reasoning questions are further divided into 10 and 14 subcategories, respectively, while Hallucination questions combine elements of both perception and reasoning.
        Annotators are instructed to include negative options to create a multiple-choice QA format, facilitating straightforward and cost-effective assessments. To minimize expression loss, annotators use their native language during the annotation process. Each video requires between 6 to 15 QAC triplets, depending on its duration.
      </div>

      <br/>
          <center><a href="resources/statistics.jpg"><img src = "resources/statistics.jpg" width="1000px"></img></a><br></center>
      <br/>
      <hr>

      <div style="width:1200px; margin:0 auto; text-align=center">
        <center><h1>Benchmark Comparison</h1></center>
      </div>
      <div style="width:1200px; margin:0 auto; text-align: left">
        CG-Bench is characterized by its diverse features, allowing it to be compared with three distinct types of benchmarks, as depicted in the three sections of Table~\ref{comparison-bebchmark-table}: Question Clue Grounding, Short-Video QA, and Long-Video QA benchmarks.
        <br>
        <b>Question Grounding:</b> \textcolor{blue}{For the question clue grounding benchmarks, NextGQA~\citep{xiao2024nextgqa}, Ego4D-NLQ~\citep{ego4d}, MultiHop-EgoQA~\citep{chen2024multihop-egoqa}, E.T. Bench~\citep{liu2024etbench}, and RexTime~\citep{chen2024rextime} are primarily centered around action and egocentric domains. Their videos are sampled from academic datasets.}
        In comparison, the question clue grounding part of CG-Bench, CG-Bench-QG, stands out with the highest number of videos and the longest average length, the diversity of which fosters a broad spectrum of question-grounding queries. 
        <br>
        <b>Short-Video Question Answering:</b> Furthermore, we transform QAC triplets to our novel Short-Video QA benchmark, termed CG-Bench-Clue. When contrasted with prior short video benchmarks such as TempCompass~\citep{liu2024tempcompass}, MVBench~\citep{mvbench} and MMBench-Video~\citep{fang2024mmbenchvideo}, our CG-Bench-Clue emerges as the \emph{largest}, \emph{held-out}, \emph{open-domain} and \emph{multimodal} Short-Video QA benchmark. 
        <br>
        <b>Long-Video Question Answering:</b> As for the Long-Video QA benchmark, CG-Bench excels in the number of videos, length, quantity of questions, and annotation quality. Owing to our clue interval annotations, CG-Bench further facilitates reliable evaluations for long videos and open-ended evaluations with clue assistance, a feature that sets it apart from existing long video benchmarks like Video-MME~\citep{videomme} and MLVU~\citep{zhou2024mlvu}.
      </div>
      <br/>
      <center><a href="resources/comparison.jpg"><img src = "resources/comparison.jpg" width="1000px"></img></a><br></center>
      <br/>
    <hr>

    <div style="width:1200px; margin:0 auto; text-align=center">
      <center><h1>Experiments Results</h1></center>
    </div>
    <center><a href="resources/experimental_results.jpg"><img src = "resources/experimental_results.jpg" width="1000px"></img></a><br></center>
    <hr>

            <table align=center width=1000px style="max-width: 100%; background: #f7fafc; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); padding: 20px; margin: 20px auto;">
              <center><h1 style="margin-bottom: 20px;">Citation</h1></center>
              <tr>
              <td width=1000px align=left>
                <div class="paper" id="mat2023_bib" style="background: #fff; border-radius: 4px; padding: 15px; position: relative;">
                  <button onclick="copyToClipboard()" style="position: absolute; top: 10px; right: 10px; padding: 5px 10px; background: #4a5568; color: white; border: none; border-radius: 4px; cursor: pointer;">Copy</button>
                  <div id="copyNotification" style="display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); background: rgba(0,0,0,0.8); color: white; padding: 10px 20px; border-radius: 4px; animation: fadeOut 2s forwards;">Citation copied to clipboard!</div>
                  <pre xml:space="preserve" style="margin: 0; white-space: pre-wrap;">@misc{chen2023cgbench,
      title={CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding}, 
      author={Guo Chen and Yicheng Liu and Yifei Huang and Yuping He and Baoqi Pei and Jilan Xu and Yali Wang and Tong Lu and Limin Wang},
      year={2024},
      eprint={xxxx.xxxxx},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</pre>
                </div>
                <style>
                @keyframes fadeOut {
                  0% { opacity: 1; }
                  70% { opacity: 1; }
                  100% { opacity: 0; }
                }
                </style>
                <script>
                function copyToClipboard() {
                  const text = document.querySelector('#mat2023_bib pre').innerText;
                  navigator.clipboard.writeText(text).then(() => {
                    const notification = document.getElementById('copyNotification');
                    notification.style.display = 'block';
                    setTimeout(() => {
                      notification.style.display = 'none';
                    }, 2000);
                  });
                }
                </script>
                </td>
                </tr>
            </table>
          <hr>


      <div style="text-align:right; font-size: 10px;">
        <a href="https://ramanans1.github.io/plan2explore/" style="color: #ccc">Website source</a>
      </div>
      </table>
</div>


<script>
  function sortTable(columnIndex) {
      const table = document.getElementById("sortableTable");
      let rows = Array.from(table.tBodies[0].rows);
      let isAscending = table.getAttribute("data-sort-order") === "asc";

      // Store original rank numbers
      const originalRanks = rows.map(row => row.cells[0].innerText);

      // Remove existing animation classes
      rows.forEach(row => row.classList.remove('row-transition'));

      // Sort the rows
      rows.sort((rowA, rowB) => {
          const cellA = rowA.cells[columnIndex].innerText;
          const cellB = rowB.cells[columnIndex].innerText;
          const numA = parseFloat(cellA);
          const numB = parseFloat(cellB);
          if (!isNaN(numA) && !isNaN(numB)) {
              return isAscending ? numA - numB : numB - numA;
          }
          return isAscending ? cellA.localeCompare(cellB) : cellB.localeCompare(cellA);
      });

      // Apply animation and reorder rows
      rows.forEach((row, index) => {
          row.classList.add('row-transition');
          row.cells[0].innerText = originalRanks[index];
          table.tBodies[0].appendChild(row);
      });

      table.setAttribute("data-sort-order", isAscending ? "desc" : "asc");

      // Update sort indicators
      const headers = table.getElementsByTagName("th");
      for (let header of headers) {
          header.classList.remove("sorted-asc", "sorted-desc");
      }
      const clickedHeader = table.rows[1].cells[columnIndex-2];
      clickedHeader.classList.add(isAscending ? "sorted-desc" : "sorted-asc");
  }

  function adjustFontSize(action) {
      const table = document.getElementById('sortableTable');
      const currentSize = parseFloat(window.getComputedStyle(table).fontSize);
      
      let newSize;
      if (action === 'increase') {
          newSize = currentSize * 1.1; // 增加10%
          if (newSize > 24) newSize = 24; // 最大字号限制
      } else {
          newSize = currentSize * 0.9; // 减少10%
          if (newSize < 10) newSize = 10; // 最小字号限制
      }
      
      // 应用到整个表格及其所有子元素
      table.style.fontSize = `${newSize}px`;
      
      // 调整表格内所有单元格的padding以保持布局比例
      const cells = table.getElementsByTagName('td');
      const headers = table.getElementsByTagName('th');
      const padding = Math.max(6, Math.floor(newSize * 0.5));
      
      [...cells, ...headers].forEach(cell => {
          cell.style.padding = `${padding}px`;
      });

      // 确保表格内的所有文本元素都继承新的字体大小
      const allElements = table.getElementsByTagName('*');
      [...allElements].forEach(element => {
          if (element.tagName !== 'IMG' && element.tagName !== 'BUTTON') {
              element.style.fontSize = `${newSize}px`;
          }
      });
  }

  // 监听元素进入视口
  document.addEventListener('DOMContentLoaded', function() {
      const elements = document.querySelectorAll('.fade-in');
      const observer = new IntersectionObserver((entries) => {
          entries.forEach(entry => {
              if (entry.isIntersecting) {
                  entry.target.classList.add('visible');
              }
          });
      });

      elements.forEach(element => observer.observe(element));
  });
</script>
</body>
</html>
