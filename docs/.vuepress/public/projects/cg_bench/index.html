<!doctype html>
<html lang="en-US">
<link rel="stylesheet" href="resources/styles.css">
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172325941-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-172325941-2');
  </script>

<div max-width=100%>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <title>CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://chenguo.netlify.app/projects/cg_bench" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="CG-Bench" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta property="og:description" content="Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang. CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta property="og:url" content="https://chenguo.netlify.app/projects/cg_bench" />

  <meta property="article:publisher" content="https://github.com/cg1177" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta name="twitter:description" content="Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, Limin Wang. CG-Bench: Clue-grounded Question Answering Benchmark &#13;for Long Video Understanding" />
  <meta name="twitter:url" content="https://chenguo.netlify.app/projects/cg_bench" />
  <meta name="twitter:site" content="@ajayj_" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">CG-Bench: Clue-grounded Question Answering Benchmark <br>for Long Video Understanding</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;">
          <div><center><span style="font-size:18px"><a href="https://chenguo.netlify.app/" target="_blank">Jiahao Wang*</a></span></center>
          <center><span style="font-size:18px">Nanjing University</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="/" target="_blank">Guo Chen*</a></span></center>
          <center><span style="font-size:18px">Nanjing University</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="https://hyf015.github.io/" target="_blank">Yifei Huang</a></span></center>
          <center><span style="font-size:18px">Shanghai AI Laboratory</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="http://wanglimin.github.io/" target="_blank">Limin Wang</a></span></center>
            <center><span style="font-size:18px">Nanjing University</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="https://cs.nju.edu.cn/lutong/index.htm" target="_blank">Tong Lu</a></span></center>
            <center><span style="font-size:18px">Nanjing University</span></center>
          </div>

            
      </div>
      <center><span style="font-size:20px;"><a href='https://iccv2023.thecvf.com/'>International Conference
        on Computer Vision (ICCV), 2023</a></span></center>

      <div class="table-like" style="justify-content:space-evenly;max-width:700px;margin:auto;padding:5px">
        <div><center><span style="font-size:28px"><a href='https://arxiv.org/abs/2308.07893'>[Paper]</a></span></center></div>
        <div><center><span style="font-size:28px"><a href='https://github.com/Echo0125/Memory-and-Anticipation-Transformer'>[GitHub Code]</a></span></center> </div>
      </div>

      <div style="width:800px; margin:0 auto; text-align=right;">
      <p>
      <b>Summary:</b> We introduce CG-Bench, a groundbreaking benchmark for clue-grounded question answering in long videos, addressing the limitations of existing benchmarks that focus primarily on short videos and rely on multiple-choice questions (MCQs). These limitations allow models to answer by elimination rather than genuine understanding. CG-Bench enhances evaluation credibility by requiring models to retrieve relevant clues for questions. It includes 1,219 manually curated videos across 14 primary, 171 secondary, and 638 tertiary categories, making it the largest benchmark for long video analysis. With 12,129 QA pairs in perception, reasoning, and hallucination question types, CG-Bench introduces innovative clue-based evaluation methods: clue-grounded white box and black box evaluations, ensuring answers are based on correct video understanding. Evaluations of various MLLMs reveal significant performance gaps in long video comprehension, especially between open-source and commercial models. We aim for CG-Bench to drive the development of more reliable and capable MLLMs for long video understanding. All annotations and video data will be publicly released.
    </p>
      <a href="resources/story_new.png"><img width="600" max-width="100%" src="resources/story_new.png" /></a>
      <p>
        <b>Abstract:</b>ost existing video understanding benchmarks for multimodal large language models (MLLMs) focus only on short videos. The limited number of benchmarks for long video understanding often rely solely on multiple-choice questions (MCQs). However, because of the inherent limitation of MCQ-based evaluation and the increasing reasoning ability of MLLMs, models can give the current answer purely by combining short video understanding with elimination, without genuinely understanding the video content.
           To address this gap, we introduce CG-Bench, a novel benchmark designed for clue-grounded question answering in long videos. CG-Bench emphasizes the model's ability to retrieve relevant clues for questions, enhancing evaluation credibility. It features 1,219 manually curated videos categorized by a granular system with 14 primary categories, 171 secondary categories, and 638 tertiary categories, making it the largest benchmark for long video analysis. The benchmark includes 12,129 QA pairs in three major question types: perception, reasoning, and hallucination.
           Compensating the drawbacks of pure MCQ-based evaluation, we design two novel clue-based evaluation methods: clue-grounded white box and black box evaluations, to assess whether the model generates answers based on the correct understanding of the video.
           We evaluate multiple closed-source and open-source MLLMs on CG-Bench. Results indicate that current models significantly underperform in understanding long videos compared to short ones, and a significant gap exists between open-source and commercial models. We hope CG-Bench can advance the development of more trustworthy and capable MLLMs for long video understanding. \textcolor{blue}{All annotations and video data will be publicly released.
      </p>
      </div>
      <center>
      <br><hr>

      <center><h1>Memory-and-Anticipation Transformer</h1></center>
      <div style="width:800px; margin:0 auto; text-align: left">
        Memory-and-Anticipation Transformer (MAT), a novel memory-anticipation-based approach that fully models the complete temporal context, including history, present, and future. A Progressive Memory Encoder is designed to provide a more precise history summary by compressing long- and short-term memory in a segment-based fashion. Meanwhile, we propose our key idea of modeling circular dependencies between memory and future, implemented as  Memory-Anticipation Circular Decoder. It first learns latent future features in a supervised manner, then updates iteratively the enhanced short-term memory and the latent future features by performing Conditional circular Interaction between them. Among them, multiple interaction processes capture the circular dependency and supervise the output to maintain stable features with real semantics. 
      </div>
      <br />
      <center><a href="resources/Framework.png"><img src = "resources/Framework.png" width="800px" ></img></a><br></center>
      <!-- <center><a href="resources/teaser_color.png"><img src = "resources/teaser_color.png" width="800px" ></img></a><br></center> -->
      <hr>


      <!-- <br />
      <hr> -->

      <div style="width:800px; margin:0 auto; text-align=center">
        <center><h1>Details of the Architecture</h1></center>
      </div>
      <div style="width:800px; margin:0 auto; text-align: left">
        Segment-based Long-term Memory Compression and Conditional Circular Interaction. MAT performs piecewise compression on long-term historical information to extract abstract features, while modeling circular dependencies among all information to form more robust semantic context.
      </div>
      <br/>
          <center><a href="resources/details.png"><img src = "resources/details.png" width="800px"></img></a><br></center>
      <br/><hr>


            <center id="sourceCode"><h1>Source Code</h1></center>
            <div style="width:800px; margin:0 auto; text-align: left">
                PyTorch code for our paper is open-source and available on GitHub. We include a efficient, pure Python implementation of MAT, as well as training and evaluation code.
            </div>
            <table align=center width=300px>
              <tr>
                <td width=300px align=center>
                  <span style="font-size:28px"><a href='https://github.com/Echo0125/Memory-and-Anticipation-Transformer'>[GitHub]</a></span>
                </td>
              </tr>
            </table>
            <br><hr>

            <table align=center width=700px style="max-width: 100%">
              <center><h1>Paper and Bibtex</h1></center>
              <tr>
              <td width=250px align=left>
              <!-- <p style="margin-top:4px;"></p> -->
              <a href="https://arxiv.org/pdf/xxxx.yyyyy.pdf" target="_blank"><img style="height:150px" src="resources/paper.png"/></a>
              <center>
              <span style="font-size:20pt"><a href="https://arxiv.org/abs/2308.07893" target="_blank">[Paper]</a></span>
              </center>
              </td>
              <td width=20px align=center>
              </td>
              <td width=250px align=left>
              <p style="text-align:left;">
                <b><span style="font-size:20pt">Citation</span></b>
                <br/>
                <span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, Tong Lu. <br><b>Memory-and-Anticipation Transformer for Online Action Understanding.</b> <br>In <em>International Conference on Computer Vision (ICCV)</em>, 2023.</span></p>
              </td>
              </tr>
              <tr>
              <td width=250px align=left>
              </td>
              <td width=50px align=center>
              </td>
              <td width=550px align=left>
                <div class="paper" id="mat2023_bib">
<pre xml:space="preserve">
@misc{chen2023cgbench,
      title={CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding}, 
      author={Guo Chen and Yicheng Liu and Yifei Huang and Yuping He and Baoqi Pei and Jilan Xu and Yali Wang and Tong Lu and Limin Wang},
      year={2024},
      eprint={xxxx.xxxxx},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</pre>
                </div>
                </td>
                </tr>
            </table>
          <hr>


      <div style="text-align:right; font-size: 10px;">
        <a href="https://ramanans1.github.io/plan2explore/" style="color: #ccc">Website source</a>
      </div>
      </table>
</div>
</body>
</html>
